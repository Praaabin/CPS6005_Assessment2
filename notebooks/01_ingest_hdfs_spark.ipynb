{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62213e60",
   "metadata": {},
   "source": [
    "# Notebook 01: Ingestion from HDFS using Spark\n",
    "\n",
    "**TerraFlow Analytics - Big Data Assessment**\n",
    "\n",
    "This notebook demonstrates distributed data processing using PySpark to load raw GTFS data from HDFS, inspect it using Spark DataFrames, and save a bronze layer for further processing.\n",
    "\n",
    "**Requirements Addressed:**\n",
    "1. **Distributed data processing with PySpark**: Loading and parsing large GTFS files.\n",
    "2. **Scalable storage with HDFS**: Reading from and writing to HDFS.\n",
    "3. **Use Spark DataFrame and RDDs**: Handling temporal and spatial attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d55cda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SPARK SESSION INITIALIZED\n",
      "============================================================\n",
      "Application Name: TerraFlow_Ingestion\n",
      "Spark Version: 3.5.0\n",
      "Master: local[*]\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Initialize Spark Session for distributed data processing\n",
    "# 'local[*]' is used here for demonstration, but in a full cluster this would connect to the Spark Master\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TerraFlow_Ingestion\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:9000\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SPARK SESSION INITIALIZED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Application Name: {spark.sparkContext.appName}\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Master: {spark.sparkContext.master}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f12ffbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDFS Configuration:\n",
      "NameNode: hdfs://namenode:9000\n",
      "Raw Data Path: hdfs://namenode:9000/terraflow/data/raw/gtfs_data.csv\n"
     ]
    }
   ],
   "source": [
    "# HDFS Configuration\n",
    "# We strictly use proper HDFS paths as per assignment requirements\n",
    "HDFS_NAMENODE = \"hdfs://namenode:9000\"\n",
    "\n",
    "# Path to the raw CSV file uploaded to HDFS\n",
    "# Note: The file 'CPS6005-Assessment 2_GTFS_Data.csv' is stored as 'gtfs_data.csv' in HDFS\n",
    "RAW_DATA_PATH = f\"{HDFS_NAMENODE}/terraflow/data/raw/gtfs_data.csv\"\n",
    "\n",
    "print(\"HDFS Configuration:\")\n",
    "print(f\"NameNode: {HDFS_NAMENODE}\")\n",
    "print(f\"Raw Data Path: {RAW_DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62377385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from HDFS...\n",
      "Data loaded successfully!\n",
      "Number of partitions: 2\n"
     ]
    }
   ],
   "source": [
    "# 1. Load data from HDFS using Spark DataFrame\n",
    "# inferSchema=True allows Spark to automatically detect data types\n",
    "print(\"Loading data from HDFS...\")\n",
    "df = spark.read.csv(RAW_DATA_PATH, header=True, inferSchema=True)\n",
    "\n",
    "print(\"Data loaded successfully!\")\n",
    "print(f\"Number of partitions: {df.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a3a62f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Schema:\n",
      "root\n",
      " |-- stop_id_from: integer (nullable = true)\n",
      " |-- stop_id_to: integer (nullable = true)\n",
      " |-- trip_id: string (nullable = true)\n",
      " |-- arrival_time: timestamp (nullable = true)\n",
      " |-- time: double (nullable = true)\n",
      " |-- speed: string (nullable = true)\n",
      " |-- Number_of_trips: integer (nullable = true)\n",
      " |-- SRI: string (nullable = true)\n",
      " |-- Degree_of_congestion: string (nullable = true)\n",
      "\n",
      "\n",
      "Total rows in dataset: 66,913\n",
      "\n",
      "Sample Data (Top 5 Rows):\n",
      "+------------+----------+------------------------------------------------------------+-------------------+-----------+-----------+---------------+-----------+--------------------+\n",
      "|stop_id_from|stop_id_to|trip_id                                                     |arrival_time       |time       |speed      |Number_of_trips|SRI        |Degree_of_congestion|\n",
      "+------------+----------+------------------------------------------------------------+-------------------+-----------+-----------+---------------+-----------+--------------------+\n",
      "|36156       |38709     |NORMAL_333_Pune Station To  Hinjawadi Maan Phase 3_Up-0855_0|2026-01-12 09:13:54|0.027222222|14.47956475|9              |-0.40816322|Very smooth         |\n",
      "|36156       |38709     |NORMAL_115P_Pune Station to Hinjawadi Phase 3_Up-0845_0     |2026-01-12 09:03:01|0.032222222|12.23273572|9              |1.2068965  |Smooth              |\n",
      "|36156       |38709     |NORMAL_100_Ma Na Pa to Hinjawadi Maan Phase 3_Up-0915_0     |2026-01-12 09:15:00|0.058333333|6.7571302  |9              |5.142857   |Heavy congestion    |\n",
      "|36156       |38709     |NORMAL_VJR5_Ma Na Pa To Mukai Chowk_Up-0905_0               |2026-01-12 09:05:00|0.033611111|11.72725073|9              |1.570248   |Smooth              |\n",
      "|36156       |38709     |NORMAL_100_Ma Na Pa to Hinjawadi Maan Phase 3_Up-0900_0     |2026-01-12 09:00:00|0.058333333|6.7571302  |9              |5.142857   |Heavy congestion    |\n",
      "+------------+----------+------------------------------------------------------------+-------------------+-----------+-----------+---------------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Inspect Schema and Row Count\n",
    "print(\"Dataset Schema:\")\n",
    "df.printSchema()\n",
    "\n",
    "total_rows = df.count()\n",
    "print(f\"\\nTotal rows in dataset: {total_rows:,}\")\n",
    "\n",
    "print(\"\\nSample Data (Top 5 Rows):\")\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93b07b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting DataFrame to RDD to demonstrate RDD handling...\n",
      "First 3 records from RDD:\n",
      "Row 1: Row(stop_id_from=36156, stop_id_to=38709, trip_id='NORMAL_333_Pune Station To  Hinjawadi Maan Phase 3_Up-0855_0', arrival_time=datetime.datetime(2026, 1, 12, 9, 13, 54), time=0.027222222, speed='14.47956475', Number_of_trips=9, SRI='-0.40816322', Degree_of_congestion='Very smooth')\n",
      "Row 2: Row(stop_id_from=36156, stop_id_to=38709, trip_id='NORMAL_115P_Pune Station to Hinjawadi Phase 3_Up-0845_0', arrival_time=datetime.datetime(2026, 1, 12, 9, 3, 1), time=0.032222222, speed='12.23273572', Number_of_trips=9, SRI='1.2068965', Degree_of_congestion='Smooth')\n",
      "Row 3: Row(stop_id_from=36156, stop_id_to=38709, trip_id='NORMAL_100_Ma Na Pa to Hinjawadi Maan Phase 3_Up-0915_0', arrival_time=datetime.datetime(2026, 1, 12, 9, 15), time=0.058333333, speed='6.7571302', Number_of_trips=9, SRI='5.142857', Degree_of_congestion='Heavy congestion')\n"
     ]
    }
   ],
   "source": [
    "# 3. Demonstrate usage of RDDs (Requirement: Use Spark DataFrame and RDDs)\n",
    "# We convert the DataFrame to an RDD and inspect the first few records\n",
    "print(\"Converting DataFrame to RDD to demonstrate RDD handling...\")\n",
    "rdd_sample = df.rdd.take(3)\n",
    "\n",
    "print(\"First 3 records from RDD:\")\n",
    "for i, row in enumerate(rdd_sample, 1):\n",
    "    print(f\"Row {i}: {row}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f845921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving bronze layer to HDFS at: hdfs://namenode:9000/terraflow/data/processed/gtfs_bronze.parquet\n",
      "✓ Bronze layer saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# 4. Save processed data back to HDFS (Bronze Layer)\n",
    "# Saving as Parquet - a columnar storage format optimized for big data analytics\n",
    "BRONZE_OUTPUT_PATH = f\"{HDFS_NAMENODE}/terraflow/data/processed/gtfs_bronze.parquet\"\n",
    "\n",
    "print(f\"Saving bronze layer to HDFS at: {BRONZE_OUTPUT_PATH}\")\n",
    "\n",
    "# mode(\"overwrite\") allows re-running this notebook safely\n",
    "df.write.mode(\"overwrite\").parquet(BRONZE_OUTPUT_PATH)\n",
    "\n",
    "print(\"✓ Bronze layer saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12e8160d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading back the saved Bronze layer to verify integrity...\n",
      "Original Count: 66,913\n",
      "Saved Count:    66,913\n",
      "\n",
      "✅ SUCCESS: Data integrity verified.\n"
     ]
    }
   ],
   "source": [
    "# 5. Verify the saved file\n",
    "print(\"Reading back the saved Bronze layer to verify integrity...\")\n",
    "df_bronze = spark.read.parquet(BRONZE_OUTPUT_PATH)\n",
    "bronze_count = df_bronze.count()\n",
    "\n",
    "print(f\"Original Count: {total_rows:,}\")\n",
    "print(f\"Saved Count:    {bronze_count:,}\")\n",
    "\n",
    "if total_rows == bronze_count:\n",
    "    print(\"\\n✅ SUCCESS: Data integrity verified.\")\n",
    "else:\n",
    "    print(\"\\n❌ FAILURE: Row counts do not match!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "463acb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session stopped.\n"
     ]
    }
   ],
   "source": [
    "# Stop Spark Session\n",
    "spark.stop()\n",
    "print(\"Spark session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
