{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62213e60",
   "metadata": {},
   "source": [
    "# Notebook 01: Ingestion from HDFS using Spark\n",
    "\n",
    "**TerraFlow Analytics - Big Data Assessment**\n",
    "\n",
    "This notebook demonstrates distributed data processing using PySpark to load raw GTFS data from HDFS, inspect it using Spark DataFrames, and save a bronze layer for further processing.\n",
    "\n",
    "**Requirements Addressed:**\n",
    "1. **Distributed data processing with PySpark**: Loading and parsing large GTFS files.\n",
    "2. **Scalable storage with HDFS**: Reading from and writing to HDFS.\n",
    "3. **Use Spark DataFrame and RDDs**: Handling temporal and spatial attributes.\n",
    "\n",
    "**FAST VERSION** - Optimized for quick execution with robust fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d55cda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Config loaded\n",
      "NameNode: hdfs://namenode:9000\n",
      "Raw Path: hdfs://namenode:9000/terraflow/data/raw/gtfs_data.csv\n",
      "Bronze  : hdfs://namenode:9000/terraflow/data/processed/gtfs_bronze.parquet\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, DoubleType\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration (HDFS + Paths)\n",
    "# -----------------------------\n",
    "HDFS_NAMENODE = \"hdfs://namenode:9000\"\n",
    "\n",
    "RAW_DATA_PATH = f\"{HDFS_NAMENODE}/terraflow/data/raw/gtfs_data.csv\"\n",
    "BRONZE_OUTPUT_PATH = f\"{HDFS_NAMENODE}/terraflow/data/processed/gtfs_bronze.parquet\"\n",
    "\n",
    "print(\"‚úÖ Config loaded\")\n",
    "print(\"NameNode:\", HDFS_NAMENODE)\n",
    "print(\"Raw Path:\", RAW_DATA_PATH)\n",
    "print(\"Bronze  :\", BRONZE_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f12ffbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Spark session...\n",
      "======================================================================\n",
      "SPARK SESSION INITIALIZED\n",
      "======================================================================\n",
      "Spark Version : 3.3.2\n",
      "defaultFS     : hdfs://namenode:9000\n",
      "Parallelism   : 4\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session (LOCAL MODE - Fast & Professional)\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "print(\"Initializing Spark session...\")\n",
    "\n",
    "# Stop any existing session\n",
    "try:\n",
    "    if 'spark' in globals() and spark is not None:\n",
    "        spark.stop()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    SparkSession._instantiatedSession = None\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"TerraFlow_Ingestion_Bronze\")\n",
    "    .master(\"local[4]\")  # 4 parallel threads for distributed processing\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:9000\")\n",
    "    \n",
    "    # Performance optimization\n",
    "    .config(\"spark.driver.memory\", \"2g\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "    .config(\"spark.default.parallelism\", \"4\")\n",
    "    \n",
    "    # HDFS connection settings\n",
    "    .config(\"spark.hadoop.dfs.client.use.datanode.hostname\", \"true\")\n",
    "    \n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SPARK SESSION INITIALIZED\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Spark Version :\", spark.version)\n",
    "print(\"defaultFS     :\", spark._jsc.hadoopConfiguration().get(\"fs.defaultFS\"))\n",
    "print(\"Parallelism   :\", spark.sparkContext.defaultParallelism)\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62377385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Checking raw folder in HDFS:\n",
      "‚úÖ HDFS listing for: hdfs://namenode:9000/terraflow/data/raw\n",
      " - hdfs://namenode:9000/terraflow/data/raw/gtfs_data.csv\n",
      "\n",
      " Checking data availability...\n",
      "‚úÖ Raw CSV found in HDFS.\n"
     ]
    }
   ],
   "source": [
    "# HDFS Helper Functions\n",
    "def hdfs_fs():\n",
    "    jvm = spark._jvm\n",
    "    hconf = spark._jsc.hadoopConfiguration()\n",
    "    return jvm.org.apache.hadoop.fs.FileSystem.get(hconf)\n",
    "\n",
    "def hdfs_exists(path: str) -> bool:\n",
    "    try:\n",
    "        fs = hdfs_fs()\n",
    "        jvm = spark._jvm\n",
    "        return fs.exists(jvm.org.apache.hadoop.fs.Path(path))\n",
    "    except Exception as e:\n",
    "        print(f\"  Error checking HDFS: {e}\")\n",
    "        return False\n",
    "\n",
    "def hdfs_list(path: str, limit: int = 20):\n",
    "    try:\n",
    "        fs = hdfs_fs()\n",
    "        jvm = spark._jvm\n",
    "        p = jvm.org.apache.hadoop.fs.Path(path)\n",
    "        if not fs.exists(p):\n",
    "            print(f\" HDFS path not found: {path}\")\n",
    "            return\n",
    "        statuses = fs.listStatus(p)\n",
    "        print(f\"‚úÖ HDFS listing for: {path}\")\n",
    "        for i, st in enumerate(statuses):\n",
    "            if i >= limit:\n",
    "                print(\"... (truncated)\")\n",
    "                break\n",
    "            print(\" -\", st.getPath().toString())\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error listing HDFS: {e}\")\n",
    "\n",
    "print(\" Checking raw folder in HDFS:\")\n",
    "hdfs_list(f\"{HDFS_NAMENODE}/terraflow/data/raw\")\n",
    "\n",
    "# ROBUST DATA LOADING STRATEGY\n",
    "print(\"\\n Checking data availability...\")\n",
    "USE_LOCAL_FALLBACK = False\n",
    "\n",
    "if hdfs_exists(RAW_DATA_PATH):\n",
    "    print(\"‚úÖ Raw CSV found in HDFS.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Raw file not found in HDFS (gtfs_data.csv). Checking local fallback...\")\n",
    "    # Check mounted volume\n",
    "    import os\n",
    "    local_path_1 = \"/home/jovyan/work/data/raw/CPS6005-Assessment 2_GTFS_Data.csv\"\n",
    "    local_path_2 = \"/home/jovyan/work/data/raw/gtfs_data.csv\"\n",
    "    \n",
    "    if os.path.exists(local_path_1):\n",
    "        RAW_DATA_PATH = f\"file://{local_path_1}\"\n",
    "        USE_LOCAL_FALLBACK = True\n",
    "        print(f\" Found local fallback: {local_path_1}\")\n",
    "    elif os.path.exists(local_path_2):\n",
    "        RAW_DATA_PATH = f\"file://{local_path_2}\"\n",
    "        USE_LOCAL_FALLBACK = True\n",
    "        print(f\"‚úÖ Found local fallback: {local_path_2}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\" Data file not found in HDFS OR local volume! Please upload data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a3a62f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Reading CSV from: hdfs://namenode:9000/terraflow/data/raw/gtfs_data.csv\n",
      "‚úÖ DataFrame loaded\n",
      "Columns: 9\n",
      "Partitions: 2\n"
     ]
    }
   ],
   "source": [
    "# Define schema for faster parsing\n",
    "schema = StructType([\n",
    "    StructField(\"stop_id_from\", IntegerType(), True),\n",
    "    StructField(\"stop_id_to\", IntegerType(), True),\n",
    "    StructField(\"trip_id\", StringType(), True),\n",
    "    StructField(\"arrival_time\", StringType(), True),\n",
    "    StructField(\"time\", DoubleType(), True),\n",
    "    StructField(\"speed\", StringType(), True),\n",
    "    StructField(\"Number_of_trips\", IntegerType(), True),\n",
    "    StructField(\"SRI\", StringType(), True),\n",
    "    StructField(\"Degree_of_congestion\", StringType(), True),\n",
    "])\n",
    "\n",
    "print(f\"üì• Reading CSV from: {RAW_DATA_PATH}\")\n",
    "df = spark.read.csv(RAW_DATA_PATH, header=True, schema=schema)\n",
    "\n",
    "print(\"‚úÖ DataFrame loaded\")\n",
    "print(\"Columns:\", len(df.columns))\n",
    "print(\"Partitions:\", df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93b07b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Schema:\n",
      "root\n",
      " |-- stop_id_from: integer (nullable = true)\n",
      " |-- stop_id_to: integer (nullable = true)\n",
      " |-- trip_id: string (nullable = true)\n",
      " |-- arrival_time: string (nullable = true)\n",
      " |-- time: double (nullable = true)\n",
      " |-- speed: string (nullable = true)\n",
      " |-- Number_of_trips: integer (nullable = true)\n",
      " |-- SRI: string (nullable = true)\n",
      " |-- Degree_of_congestion: string (nullable = true)\n",
      "\n",
      "\n",
      "‚úÖ Sample rows:\n",
      "+------------+----------+------------------------------------------------------------+------------+-----------+-----------+---------------+-----------+--------------------+\n",
      "|stop_id_from|stop_id_to|trip_id                                                     |arrival_time|time       |speed      |Number_of_trips|SRI        |Degree_of_congestion|\n",
      "+------------+----------+------------------------------------------------------------+------------+-----------+-----------+---------------+-----------+--------------------+\n",
      "|36156       |38709     |NORMAL_333_Pune Station To  Hinjawadi Maan Phase 3_Up-0855_0|09:13:54    |0.027222222|14.47956475|9              |-0.40816322|Very smooth         |\n",
      "|36156       |38709     |NORMAL_115P_Pune Station to Hinjawadi Phase 3_Up-0845_0     |09:03:01    |0.032222222|12.23273572|9              |1.2068965  |Smooth              |\n",
      "|36156       |38709     |NORMAL_100_Ma Na Pa to Hinjawadi Maan Phase 3_Up-0915_0     |09:15:00    |0.058333333|6.7571302  |9              |5.142857   |Heavy congestion    |\n",
      "|36156       |38709     |NORMAL_VJR5_Ma Na Pa To Mukai Chowk_Up-0905_0               |09:05:00    |0.033611111|11.72725073|9              |1.570248   |Smooth              |\n",
      "|36156       |38709     |NORMAL_100_Ma Na Pa to Hinjawadi Maan Phase 3_Up-0900_0     |09:00:00    |0.058333333|6.7571302  |9              |5.142857   |Heavy congestion    |\n",
      "+------------+----------+------------------------------------------------------------+------------+-----------+-----------+---------------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "‚úÖ Total rows: 66,913\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n‚úÖ Schema:\")\n",
    "df.printSchema()\n",
    "\n",
    "print(\"\\n‚úÖ Sample rows:\")\n",
    "df.show(5, truncate=False)\n",
    "\n",
    "# Cache for performance\n",
    "df = df.cache()\n",
    "total_rows = df.count()\n",
    "\n",
    "print(f\"\\n‚úÖ Total rows: {total_rows:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f845921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Null counts (key columns):\n",
      "speed: 258\n",
      "SRI: 313\n",
      "Degree_of_congestion: 0\n"
     ]
    }
   ],
   "source": [
    "# Null check on key columns\n",
    "key_cols = [\"speed\", \"SRI\", \"Degree_of_congestion\"]\n",
    "nulls = (\n",
    "    df.select([F.sum(F.col(c).isNull().cast(\"int\")).alias(c) for c in key_cols])\n",
    "      .collect()[0]\n",
    "      .asDict()\n",
    ")\n",
    "\n",
    "print(\"\\n Null counts (key columns):\")\n",
    "for k, v in nulls.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12e8160d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "RDD OPERATIONS \n",
      "======================================================================\n",
      "\n",
      " RDD created from DataFrame\n",
      "   RDD partitions: 2\n",
      "\n",
      " RDD Operation 1: take(3) - First 3 rows:\n",
      "   Row 1: Row(stop_id_from=36156, stop_id_to=38709, trip_id='NORMAL_333_Pune Station To  Hinjawadi Maan Phase 3_Up-0855_0', arrival_time='09:13:54', time=0.027222222, speed='14.47956475', Number_of_trips=9, SRI='-0.40816322', Degree_of_congestion='Very smooth')\n",
      "   Row 2: Row(stop_id_from=36156, stop_id_to=38709, trip_id='NORMAL_115P_Pune Station to Hinjawadi Phase 3_Up-0845_0', arrival_time='09:03:01', time=0.032222222, speed='12.23273572', Number_of_trips=9, SRI='1.2068965', Degree_of_congestion='Smooth')\n",
      "   Row 3: Row(stop_id_from=36156, stop_id_to=38709, trip_id='NORMAL_100_Ma Na Pa to Hinjawadi Maan Phase 3_Up-0915_0', arrival_time='09:15:00', time=0.058333333, speed='6.7571302', Number_of_trips=9, SRI='5.142857', Degree_of_congestion='Heavy congestion')\n",
      "\n",
      " RDD Operation 2: mapPartitions() - Count rows per partition:\n",
      "   Partition 0: 34,373 rows\n",
      "   Partition 1: 32,540 rows\n",
      "\n",
      " RDD Operation 3: filter() - Count high congestion records:\n",
      "   High congestion records: 0\n",
      "\n",
      "======================================================================\n",
      " RDD operations demonstrated successfully!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# RDD Operations \n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RDD OPERATIONS \")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "rdd = df.rdd\n",
    "print(\"\\n RDD created from DataFrame\")\n",
    "print(\"   RDD partitions:\", rdd.getNumPartitions())\n",
    "\n",
    "# RDD Operation 1: Take sample rows\n",
    "sample_rows = rdd.take(3)\n",
    "print(\"\\n RDD Operation 1: take(3) - First 3 rows:\")\n",
    "for i, row in enumerate(sample_rows, 1):\n",
    "    print(f\"   Row {i}: {row}\")\n",
    "\n",
    "# RDD Operation 2: Map transformation\n",
    "print(\"\\n RDD Operation 2: mapPartitions() - Count rows per partition:\")\n",
    "part_sizes = rdd.mapPartitions(lambda it: [sum(1 for _ in it)]).collect()\n",
    "for i, size in enumerate(part_sizes):\n",
    "    print(f\"   Partition {i}: {size:,} rows\")\n",
    "\n",
    "# RDD Operation 3: Filter transformation\n",
    "print(\"\\n RDD Operation 3: filter() - Count high congestion records:\")\n",
    "high_congestion_count = rdd.filter(\n",
    "    lambda row: row.Degree_of_congestion == \"High\" if row.Degree_of_congestion else False\n",
    ").count()\n",
    "print(f\"   High congestion records: {high_congestion_count:,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" RDD operations demonstrated successfully!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "463acb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Writing Bronze dataset to HDFS: hdfs://namenode:9000/terraflow/data/processed/gtfs_bronze.parquet\n",
      " Bronze write complete\n",
      "\n",
      " Listing processed folder:\n",
      "‚úÖ HDFS listing for: hdfs://namenode:9000/terraflow/data/processed\n",
      " - hdfs://namenode:9000/terraflow/data/processed/gtfs_bronze.parquet\n",
      " - hdfs://namenode:9000/terraflow/data/processed/gtfs_silver.parquet\n",
      " - hdfs://namenode:9000/terraflow/data/processed/route_stats.parquet\n",
      "\n",
      " Bronze sample:\n",
      "+------------+----------+------------------------------------------------------------------+------------+-----------+-----------+---------------+------------+--------------------+\n",
      "|stop_id_from|stop_id_to|trip_id                                                           |arrival_time|time       |speed      |Number_of_trips|SRI         |Degree_of_congestion|\n",
      "+------------+----------+------------------------------------------------------------------+------------+-----------+-----------+---------------+------------+--------------------+\n",
      "|34871       |33703     |NORMAL_24_Katraj To Maharashtra Housing Board Samtanagar_Up-1250_0|14:02:28    |0.0025     |60.42402253|5              |-21.11111199|Very smooth         |\n",
      "|33703       |35643     |NORMAL_24_Katraj To Maharashtra Housing Board Samtanagar_Up-1350_0|14:55:35    |0.014722222|20.86611004|5              |8.490566166 |Heavy congestion    |\n",
      "|33703       |35643     |NORMAL_24_Katraj To Maharashtra Housing Board Samtanagar_Up-1330_0|14:35:35    |0.014722222|20.86611004|5              |8.490566166 |Heavy congestion    |\n",
      "|33703       |35643     |NORMAL_24_Katraj To Maharashtra Housing Board Samtanagar_Up-1345_0|14:50:35    |0.014722222|20.86611004|5              |8.490566166 |Heavy congestion    |\n",
      "|33703       |35643     |NORMAL_24_Katraj To Maharashtra Housing Board Samtanagar_Up-1310_0|14:22:48    |0.015555556|19.74828186|5              |8.571428755 |Heavy congestion    |\n",
      "+------------+----------+------------------------------------------------------------------+------------+-----------+-----------+---------------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "======================================================================\n",
      "BRONZE LAYER VERIFICATION\n",
      "======================================================================\n",
      "Raw rows   : 66,913\n",
      "Bronze rows: 66,913\n",
      " SUCCESS: Bronze output matches raw row count.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Writing Bronze dataset to HDFS:\", BRONZE_OUTPUT_PATH)\n",
    "\n",
    "# Write to HDFS in Parquet format\n",
    "(\n",
    "    df.coalesce(4)\n",
    "      .write\n",
    "      .mode(\"overwrite\")\n",
    "      .parquet(BRONZE_OUTPUT_PATH)\n",
    ")\n",
    "\n",
    "print(\" Bronze write complete\")\n",
    "\n",
    "print(\"\\n Listing processed folder:\")\n",
    "hdfs_list(f\"{HDFS_NAMENODE}/terraflow/data/processed\")\n",
    "\n",
    "if not hdfs_exists(BRONZE_OUTPUT_PATH):\n",
    "    raise RuntimeError(\" Bronze output not found in HDFS after write\")\n",
    "\n",
    "# Verify by reading back\n",
    "df_bronze = spark.read.parquet(BRONZE_OUTPUT_PATH)\n",
    "\n",
    "print(\"\\n Bronze sample:\")\n",
    "df_bronze.show(5, truncate=False)\n",
    "\n",
    "bronze_rows = df_bronze.count()\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"BRONZE LAYER VERIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Raw rows   : {total_rows:,}\")\n",
    "print(f\"Bronze rows: {bronze_rows:,}\")\n",
    "\n",
    "if bronze_rows == total_rows:\n",
    "    print(\" SUCCESS: Bronze output matches raw row count.\")\n",
    "else:\n",
    "    print(\" WARNING: Row counts differ\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1909da48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "‚úÖ NOTEBOOK 01 COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "Summary:\n",
      "  ‚Ä¢ Loaded 66,913 records from HDFS\n",
      "  ‚Ä¢ Created Bronze layer in Parquet format\n",
      "  ‚Ä¢ Demonstrated DataFrame operations\n",
      "  ‚Ä¢ Demonstrated RDD operations\n",
      "  ‚Ä¢ Used HDFS for scalable storage\n",
      "\n",
      "‚û°Ô∏è  Proceed to Notebook 02: Data Cleaning\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ NOTEBOOK 01 COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"  ‚Ä¢ Loaded {total_rows:,} records from HDFS\")\n",
    "print(f\"  ‚Ä¢ Created Bronze layer in Parquet format\")\n",
    "print(f\"  ‚Ä¢ Demonstrated DataFrame operations\")\n",
    "print(f\"  ‚Ä¢ Demonstrated RDD operations\")\n",
    "print(f\"  ‚Ä¢ Used HDFS for scalable storage\")\n",
    "print(\"\\n‚û°Ô∏è  Proceed to Notebook 02: Data Cleaning\")\n",
    "print(\"=\" * 70)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
