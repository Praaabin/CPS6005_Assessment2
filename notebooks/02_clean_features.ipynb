{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "145c465e",
   "metadata": {},
   "source": [
    "# Notebook 02: Cleaning & Feature Engineering\n",
    "\n",
    "**TerraFlow Analytics - Big Data Assessment**\n",
    "\n",
    "This notebook focuses on processing the raw \"bronze\" data into a clean \"silver\" dataset. It addresses the requirements for data cleaning, structureing, and feature engineering to support downstream analysis and machine learning.\n",
    "\n",
    "**Objectives:**\n",
    "1. **Data Cleaning**: Handle missing values, fix data types, and remove invalid records.\n",
    "2. **Feature Engineering**: Create new variables for analysis (Peak/Off-Peak, Congestion Levels, Temporal Features).\n",
    "3. **Reliability Analysis**: Engineer trip reliability indicators based on SRI.\n",
    "4. **Save Silver Layer**: Store the processed dataset back to HDFS for efficient querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "config_paths",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Configuration loaded\n",
      "Bronze Input : hdfs://namenode:9000/terraflow/data/processed/gtfs_bronze.parquet\n",
      "Silver Output: hdfs://namenode:9000/terraflow/data/processed/gtfs_silver.parquet\n",
      "Route Stats  : hdfs://namenode:9000/terraflow/data/processed/route_stats.parquet\n"
     ]
    }
   ],
   "source": [
    "# Configuration - Define HDFS Paths\n",
    "# These paths align with the data pipeline from Notebook 01\n",
    "\n",
    "HDFS_NAMENODE = \"hdfs://namenode:9000\"\n",
    "\n",
    "# Input: Bronze layer from Notebook 01\n",
    "BRONZE_INPUT_PATH = f\"{HDFS_NAMENODE}/terraflow/data/processed/gtfs_bronze.parquet\"\n",
    "\n",
    "# Output: Silver layer (cleaned and feature-engineered data)\n",
    "SILVER_OUTPUT_PATH = f\"{HDFS_NAMENODE}/terraflow/data/processed/gtfs_silver.parquet\"\n",
    "\n",
    "# Output: Route-level statistics for dashboards\n",
    "ROUTE_STATS_OUTPUT_PATH = f\"{HDFS_NAMENODE}/terraflow/data/processed/route_stats.parquet\"\n",
    "\n",
    "print(\"âœ… Configuration loaded\")\n",
    "print(f\"Bronze Input : {BRONZE_INPUT_PATH}\")\n",
    "print(f\"Silver Output: {SILVER_OUTPUT_PATH}\")\n",
    "print(f\"Route Stats  : {ROUTE_STATS_OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28b8b5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Spark session...\n",
      "======================================================================\n",
      "[SUCCESS] SPARK SESSION INITIALIZED\n",
      "======================================================================\n",
      "Spark Version : 3.3.2\n",
      "defaultFS     : hdfs://namenode:9000\n",
      "Parallelism   : 4\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "print(\"Initializing Spark session...\")\n",
    "\n",
    "# Stop any existing session\n",
    "try:\n",
    "    if 'spark' in globals() and spark is not None:\n",
    "        spark.stop()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    SparkSession._instantiatedSession = None\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"TerraFlow_DataCleaning\")\n",
    "    .master(\"local[4]\")  # 4 parallel threads for distributed processing\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:9000\")\n",
    "    \n",
    "    # Performance optimization\n",
    "    .config(\"spark.driver.memory\", \"2g\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "    .config(\"spark.default.parallelism\", \"4\")\n",
    "    \n",
    "    # HDFS connection settings\n",
    "    .config(\"spark.hadoop.dfs.client.use.datanode.hostname\", \"true\")\n",
    "    \n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"[SUCCESS] SPARK SESSION INITIALIZED\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Spark Version :\", spark.version)\n",
    "print(\"defaultFS     :\", spark._jsc.hadoopConfiguration().get(\"fs.defaultFS\"))\n",
    "print(\"Parallelism   :\", spark.sparkContext.defaultParallelism)\n",
    "print(\"=\" * 70)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2879014e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Loading Bronze Data from HDFS...\n",
      "\n",
      "================================================================================\n",
      "DATA SCHEMA\n",
      "================================================================================\n",
      "root\n",
      " |-- stop_id_from: integer (nullable = true)\n",
      " |-- stop_id_to: integer (nullable = true)\n",
      " |-- trip_id: string (nullable = true)\n",
      " |-- arrival_time: string (nullable = true)\n",
      " |-- time: double (nullable = true)\n",
      " |-- speed: string (nullable = true)\n",
      " |-- Number_of_trips: integer (nullable = true)\n",
      " |-- SRI: string (nullable = true)\n",
      " |-- Degree_of_congestion: string (nullable = true)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "SAMPLE RECORDS (First 5 rows)\n",
      "================================================================================\n",
      "+------------+----------+------------------------------------------------------------------+------------+-----------+-----------+---------------+------------+--------------------+\n",
      "|stop_id_from|stop_id_to|trip_id                                                           |arrival_time|time       |speed      |Number_of_trips|SRI         |Degree_of_congestion|\n",
      "+------------+----------+------------------------------------------------------------------+------------+-----------+-----------+---------------+------------+--------------------+\n",
      "|34871       |33703     |NORMAL_24_Katraj To Maharashtra Housing Board Samtanagar_Up-1250_0|14:02:28    |0.0025     |60.42402253|5              |-21.11111199|Very smooth         |\n",
      "|33703       |35643     |NORMAL_24_Katraj To Maharashtra Housing Board Samtanagar_Up-1350_0|14:55:35    |0.014722222|20.86611004|5              |8.490566166 |Heavy congestion    |\n",
      "|33703       |35643     |NORMAL_24_Katraj To Maharashtra Housing Board Samtanagar_Up-1330_0|14:35:35    |0.014722222|20.86611004|5              |8.490566166 |Heavy congestion    |\n",
      "|33703       |35643     |NORMAL_24_Katraj To Maharashtra Housing Board Samtanagar_Up-1345_0|14:50:35    |0.014722222|20.86611004|5              |8.490566166 |Heavy congestion    |\n",
      "|33703       |35643     |NORMAL_24_Katraj To Maharashtra Housing Board Samtanagar_Up-1310_0|14:22:48    |0.015555556|19.74828186|5              |8.571428755 |Heavy congestion    |\n",
      "+------------+----------+------------------------------------------------------------------+------------+-----------+-----------+---------------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "âœ… Data loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# 1. Load Bronze Data\n",
    "print(\"ðŸ“Š Loading Bronze Data from HDFS...\")\n",
    "df = spark.read.parquet(BRONZE_INPUT_PATH)\n",
    "\n",
    "# Display Schema\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA SCHEMA\")\n",
    "print(\"=\"*80)\n",
    "df.printSchema()\n",
    "\n",
    "# Show sample records\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE RECORDS (First 5 rows)\")\n",
    "print(\"=\"*80)\n",
    "df.show(5, truncate=False)\n",
    "\n",
    "print(\"âœ… Data loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93eb9db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Initial record count: 66,913\n",
      "\n",
      "================================================================================\n",
      "DATA QUALITY CHECK - Missing Values\n",
      "================================================================================\n",
      "-RECORD 0-------------------\n",
      " stop_id_from         | 0   \n",
      " stop_id_to           | 0   \n",
      " trip_id              | 0   \n",
      " arrival_time         | 269 \n",
      " time                 | 0   \n",
      " speed                | 258 \n",
      " Number_of_trips      | 1   \n",
      " SRI                  | 313 \n",
      " Degree_of_congestion | 0   \n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'to_timestamp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 25\u001b[0m\n\u001b[1;32m     18\u001b[0m missing_counts\u001b[38;5;241m.\u001b[39mshow(vertical\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Convert columns to appropriate types\u001b[39;00m\n\u001b[1;32m     21\u001b[0m df_clean \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeed\u001b[39m\u001b[38;5;124m\"\u001b[39m, col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcast(DoubleType())) \\\n\u001b[1;32m     22\u001b[0m              \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSRI\u001b[39m\u001b[38;5;124m\"\u001b[39m, col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSRI\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcast(DoubleType())) \\\n\u001b[1;32m     23\u001b[0m              \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m\"\u001b[39m, col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcast(DoubleType())) \\\n\u001b[1;32m     24\u001b[0m              \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber_of_trips\u001b[39m\u001b[38;5;124m\"\u001b[39m, col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber_of_trips\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcast(IntegerType())) \\\n\u001b[0;32m---> 25\u001b[0m              \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marrival_time\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mto_timestamp\u001b[49m(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marrival_time\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Handle Missing Values (Drop rows where critical metrics are null)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m df_clean \u001b[38;5;241m=\u001b[39m df_clean\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marrival_time\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSRI\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'to_timestamp' is not defined"
     ]
    }
   ],
   "source": [
    "# 2. Data Cleaning & Type Casting\n",
    "from pyspark.sql.functions import col, sum as spark_sum, when\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "\n",
    "# Get initial count\n",
    "initial_count = df.count()\n",
    "print(f\"\\nðŸ“Š Initial record count: {initial_count:,}\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA QUALITY CHECK - Missing Values\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "missing_counts = df.select([\n",
    "    spark_sum(when(col(c).isNull(), 1).otherwise(0)).alias(c) \n",
    "    for c in df.columns\n",
    "])\n",
    "missing_counts.show(vertical=True)\n",
    "\n",
    "# Convert columns to appropriate types\n",
    "df_clean = df.withColumn(\"speed\", col(\"speed\").cast(DoubleType())) \\\n",
    "             .withColumn(\"SRI\", col(\"SRI\").cast(DoubleType())) \\\n",
    "             .withColumn(\"time\", col(\"time\").cast(DoubleType())) \\\n",
    "             .withColumn(\"Number_of_trips\", col(\"Number_of_trips\").cast(IntegerType())) \\\n",
    "             .withColumn(\"arrival_time\", to_timestamp(col(\"arrival_time\")))\n",
    "\n",
    "# Handle Missing Values (Drop rows where critical metrics are null)\n",
    "df_clean = df_clean.dropna(subset=[\"speed\", \"arrival_time\", \"SRI\"])\n",
    "after_null_drop = df_clean.count()\n",
    "\n",
    "# Remove Invalid Rows (Negative speed or time)\n",
    "df_clean = df_clean.filter((col(\"speed\") >= 0) & (col(\"time\") >= 0))\n",
    "final_count = df_clean.count()\n",
    "\n",
    "# Display cleaning summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA CLEANING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Initial records:              {initial_count:,}\")\n",
    "print(f\"After removing nulls:         {after_null_drop:,} (removed: {initial_count - after_null_drop:,})\")\n",
    "print(f\"After removing invalid data:  {final_count:,} (removed: {after_null_drop - final_count:,})\")\n",
    "print(f\"Total records removed:        {initial_count - final_count:,}\")\n",
    "print(f\"Data retention rate:          {(final_count/initial_count)*100:.2f}%\")\n",
    "\n",
    "print(\"\\nâœ… Data cleaning completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1415ac5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Feature Engineering\n",
    "from pyspark.sql.functions import col, when, hour, lit\n",
    "\n",
    "# A. Temporal Features (Hour of Day)\n",
    "df_features = df_clean.withColumn(\"hour\", hour(\"arrival_time\"))\n",
    "\n",
    "# B. Peak vs Off-Peak Classification\n",
    "# Assuming Peak Hours: 07:00-11:00 (Morning) and 16:00-20:00 (Evening)\n",
    "df_features = df_features.withColumn(\n",
    "    \"is_peak\", \n",
    "    when(((col(\"hour\") >= 7) & (col(\"hour\") <= 11)) | \n",
    "         ((col(\"hour\") >= 16) & (col(\"hour\") <= 20)), \n",
    "         lit(\"Peak\")\n",
    "    ).otherwise(lit(\"Off-Peak\"))\n",
    ")\n",
    "\n",
    "# C. Congestion Encoding (Ordinal Encoding)\n",
    "df_features = df_features.withColumn(\n",
    "    \"congestion_lebel_encoded\",\n",
    "    when(col(\"Degree_of_congestion\") == \"Very smooth\", 0)\n",
    "    .when(col(\"Degree_of_congestion\") == \"Smooth\", 1)\n",
    "    .when(col(\"Degree_of_congestion\") == \"Moderate\", 2)\n",
    "    .when(col(\"Degree_of_congestion\") == \"Heavy congestion\", 3)\n",
    "    .otherwise(4) # Unknown or Extreme\n",
    ")\n",
    "\n",
    "# D. Speed Bands (Categorical Binning)\n",
    "df_features = df_features.withColumn(\n",
    "    \"speed_band\",\n",
    "    when(col(\"speed\") < 10, \"Low (<10 km/h)\")\n",
    "    .when((col(\"speed\") >= 10) & (col(\"speed\") < 30), \"Medium (10-30 km/h)\")\n",
    "    .otherwise(\"High (>30 km/h)\")\n",
    ")\n",
    "\n",
    "# E. Trip Reliability Indicators (Requirement: trip reliability indicators)\n",
    "df_features = df_features.withColumn(\n",
    "    \"reliability_status\",\n",
    "    when(col(\"SRI\") > 2, \"Unreliable (Congested)\")\n",
    "    .otherwise(\"Reliable\")\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE ENGINEERING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show distribution of new features\n",
    "print(\"\\n Peak vs Off-Peak Distribution:\")\n",
    "df_features.groupBy(\"is_peak\").count().orderBy(\"is_peak\").show()\n",
    "\n",
    "print(\"\\n Speed Band Distribution:\")\n",
    "df_features.groupBy(\"speed_band\").count().orderBy(\"speed_band\").show()\n",
    "\n",
    "print(\"\\n Reliability Status Distribution:\")\n",
    "df_features.groupBy(\"reliability_status\").count().orderBy(\"reliability_status\").show()\n",
    "\n",
    "print(\"\\n Congestion Level Distribution:\")\n",
    "df_features.groupBy(\"Degree_of_congestion\", \"congestion_lebel_encoded\").count().orderBy(\"congestion_lebel_encoded\").show()\n",
    "\n",
    "# Show sample with new features\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE DATA WITH NEW FEATURES\")\n",
    "print(\"=\"*80)\n",
    "df_features.select(\n",
    "    \"trip_id\", \"arrival_time\", \"hour\", \"is_peak\", \n",
    "    \"speed\", \"speed_band\", \"SRI\", \"reliability_status\",\n",
    "    \"Degree_of_congestion\", \"congestion_lebel_encoded\"\n",
    ").show(10, truncate=False)\n",
    "\n",
    "print(\"âœ… Features Engineered successfully (including Reliability Indicators).\")\n",
    "# Cache the result\n",
    "df_features = df_features.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a66eb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Route Level Aggregation (Requirement: route-level aggregates)\n",
    "from pyspark.sql.functions import avg, count, stddev, col\n",
    "\n",
    "# Calculating stats per route to save as a separate dataset for dashboards\n",
    "route_stats = df_features.groupBy(\"trip_id\").agg(\n",
    "    avg(\"speed\").alias(\"avg_speed\"),\n",
    "    avg(\"SRI\").alias(\"avg_sri\"),\n",
    "    stddev(\"SRI\").alias(\"sri_volatility\"),\n",
    "    count(\"*\").alias(\"total_records\")\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ROUTE-LEVEL STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show summary statistics\n",
    "print(\"\\n Overall Route Statistics Summary:\")\n",
    "route_stats.describe().show()\n",
    "\n",
    "# Show top 10 routes by average speed\n",
    "print(\"\\n Top 10 Routes by Average Speed:\")\n",
    "route_stats.orderBy(col(\"avg_speed\").desc()).show(10)\n",
    "\n",
    "# Show top 10 routes by SRI (most unreliable)\n",
    "print(\"\\n Top 10 Most Unreliable Routes (Highest SRI):\")\n",
    "route_stats.orderBy(col(\"avg_sri\").desc()).show(10)\n",
    "\n",
    "print(\"âœ… Route aggregation completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cb1ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Save Datasets to HDFS\n",
    "\n",
    "# Save Silver Layer (Main Dataset)\n",
    "print(f\"\\n Saving Silver Dataset to HDFS: {SILVER_OUTPUT_PATH}\")\n",
    "df_features.write.mode(\"overwrite\").partitionBy(\"is_peak\").parquet(SILVER_OUTPUT_PATH)\n",
    "print(\"âœ… Silver layer saved successfully.\")\n",
    "\n",
    "# Save Route Stats (Aggregated Dataset for Dashboard)\n",
    "print(f\"\\n Saving Route Stats to HDFS: {ROUTE_STATS_OUTPUT_PATH}\")\n",
    "route_stats.write.mode(\"overwrite\").parquet(ROUTE_STATS_OUTPUT_PATH)\n",
    "print(\"âœ… Route Stats saved successfully.\")\n",
    "\n",
    "# Verification - Read back and show counts\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VERIFICATION - Reading saved data\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "silver_verify = spark.read.parquet(SILVER_OUTPUT_PATH)\n",
    "route_verify = spark.read.parquet(ROUTE_STATS_OUTPUT_PATH)\n",
    "\n",
    "print(f\"\\nâœ… Silver dataset record count: {silver_verify.count():,}\")\n",
    "print(f\"âœ… Route stats record count: {route_verify.count():,}\")\n",
    "\n",
    "print(\"\\n Silver Dataset Partitions:\")\n",
    "silver_verify.groupBy(\"is_peak\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ad128b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Completion Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"NOTEBOOK 02 COMPLETION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"âœ… Data loaded from Bronze layer\")\n",
    "print(\"âœ… Data cleaning completed (type casting, null handling, invalid data removal)\")\n",
    "print(\"âœ… Feature engineering completed:\")\n",
    "print(\"   - Temporal features (hour)\")\n",
    "print(\"   - Peak/Off-Peak classification\")\n",
    "print(\"   - Congestion level encoding\")\n",
    "print(\"   - Speed bands\")\n",
    "print(\"   - Trip reliability indicators\")\n",
    "print(\"âœ… Route-level aggregations computed\")\n",
    "print(\"âœ… Silver dataset saved to HDFS (partitioned by is_peak)\")\n",
    "print(\"âœ… Route statistics saved to HDFS\")\n",
    "print(\"\\nStopping Spark session...\")\n",
    "spark.stop()\n",
    "print(\"âœ… Spark session stopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b00b5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 6. Verification\n",
    "print(\"âœ… Notebook 02 Complete. Stopping Spark.\")\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
