{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "145c465e",
   "metadata": {},
   "source": [
    "# Notebook 02: Cleaning & Feature Engineering\n",
    "\n",
    "**TerraFlow Analytics - Big Data Assessment**\n",
    "\n",
    "This notebook focuses on processing the raw \"bronze\" data into a clean \"silver\" dataset. It addresses the requirements for data cleaning, structureing, and feature engineering to support downstream analysis and machine learning.\n",
    "\n",
    "**Objectives:**\n",
    "1. **Data Cleaning**: Handle missing values, fix data types, and remove invalid records.\n",
    "2. **Feature Engineering**: Create new variables for analysis (Peak/Off-Peak, Congestion Levels, Temporal Features).\n",
    "3. **Reliability Analysis**: Engineer trip reliability indicators based on SRI.\n",
    "4. **Save Silver Layer**: Store the processed dataset back to HDFS for efficient querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28b8b5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session Initialized\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, hour, avg, count, lit, to_timestamp, stddev\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TerraFlow_DataCleaning\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:9000\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Session Initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2879014e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from: hdfs://namenode:9000/terraflow/data/processed/gtfs_bronze.parquet\n"
     ]
    }
   ],
   "source": [
    "# Configuration Paths\n",
    "HDFS_NAMENODE = \"hdfs://namenode:9000\"\n",
    "BRONZE_INPUT_PATH = f\"{HDFS_NAMENODE}/terraflow/data/processed/gtfs_bronze.parquet\"\n",
    "SILVER_OUTPUT_PATH = f\"{HDFS_NAMENODE}/terraflow/data/processed/gtfs_silver.parquet\"\n",
    "ROUTE_STATS_OUTPUT_PATH = f\"{HDFS_NAMENODE}/terraflow/data/processed/route_stats.parquet\"\n",
    "\n",
    "print(f\"Reading data from: {BRONZE_INPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93eb9db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Row Count: 66,913\n"
     ]
    }
   ],
   "source": [
    "# 1. Load Bronze Data\n",
    "df = spark.read.parquet(BRONZE_INPUT_PATH)\n",
    "initial_count = df.count()\n",
    "\n",
    "print(f\"Initial Row Count: {initial_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1415ac5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Row Count: 66,437\n",
      "Rows Dropped: 476\n"
     ]
    }
   ],
   "source": [
    "# 2. Data Cleaning & Type Casting\n",
    "\n",
    "# Convert columns to appropriate types\n",
    "df_clean = df.withColumn(\"speed\", col(\"speed\").cast(DoubleType())) \\\n",
    "             .withColumn(\"SRI\", col(\"SRI\").cast(DoubleType())) \\\n",
    "             .withColumn(\"time\", col(\"time\").cast(DoubleType())) \\\n",
    "             .withColumn(\"Number_of_trips\", col(\"Number_of_trips\").cast(IntegerType())) \\\n",
    "             .withColumn(\"arrival_time\", to_timestamp(col(\"arrival_time\")))\n",
    "\n",
    "# Handle Missing Values (Drop rows where critical metrics are null)\n",
    "df_clean = df_clean.dropna(subset=[\"speed\", \"arrival_time\", \"SRI\"])\n",
    "\n",
    "# Remove Invalid Rows (Negative speed or time)\n",
    "df_clean = df_clean.filter((col(\"speed\") >= 0) & (col(\"time\") >= 0))\n",
    "\n",
    "cleaned_count = df_clean.count()\n",
    "dropped_count = initial_count - cleaned_count\n",
    "\n",
    "print(f\"Cleaned Row Count: {cleaned_count:,}\")\n",
    "print(f\"Rows Dropped: {dropped_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a66eb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Engineered successfully (including Reliability Indicators).\n",
      "+-------------------+--------+------------+----------------------+-------------------+\n",
      "|arrival_time       |is_peak |SRI         |reliability_status    |speed_band         |\n",
      "+-------------------+--------+------------+----------------------+-------------------+\n",
      "|2026-01-12 14:02:28|Off-Peak|-21.11111199|Reliable              |High (>30 km/h)    |\n",
      "|2026-01-12 14:55:35|Off-Peak|8.490566166 |Unreliable (Congested)|Medium (10-30 km/h)|\n",
      "|2026-01-12 14:35:35|Off-Peak|8.490566166 |Unreliable (Congested)|Medium (10-30 km/h)|\n",
      "|2026-01-12 14:50:35|Off-Peak|8.490566166 |Unreliable (Congested)|Medium (10-30 km/h)|\n",
      "|2026-01-12 14:22:48|Off-Peak|8.571428755 |Unreliable (Congested)|Medium (10-30 km/h)|\n",
      "+-------------------+--------+------------+----------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Feature Engineering\n",
    "\n",
    "# A. Temporal Features (Hour of Day)\n",
    "df_features = df_clean.withColumn(\"hour\", hour(\"arrival_time\"))\n",
    "\n",
    "# B. Peak vs Off-Peak Classification\n",
    "# Assuming Peak Hours: 07:00-11:00 (Morning) and 16:00-20:00 (Evening)\n",
    "df_features = df_features.withColumn(\n",
    "    \"is_peak\", \n",
    "    when(((col(\"hour\") >= 7) & (col(\"hour\") <= 11)) | \n",
    "         ((col(\"hour\") >= 16) & (col(\"hour\") <= 20)), \n",
    "         lit(\"Peak\")\n",
    "    ).otherwise(lit(\"Off-Peak\"))\n",
    ")\n",
    "\n",
    "# C. Congestion Encoding (Ordinal Encoding)\n",
    "df_features = df_features.withColumn(\n",
    "    \"congestion_lebel_encoded\",\n",
    "    when(col(\"Degree_of_congestion\") == \"Very smooth\", 0)\n",
    "    .when(col(\"Degree_of_congestion\") == \"Smooth\", 1)\n",
    "    .when(col(\"Degree_of_congestion\") == \"Moderate\", 2)\n",
    "    .when(col(\"Degree_of_congestion\") == \"Heavy congestion\", 3)\n",
    "    .otherwise(4) # Unknown or Extreme\n",
    ")\n",
    "\n",
    "# D. Speed Bands (Categorical Binning)\n",
    "df_features = df_features.withColumn(\n",
    "    \"speed_band\",\n",
    "    when(col(\"speed\") < 10, \"Low (<10 km/h)\")\n",
    "    .when((col(\"speed\") >= 10) & (col(\"speed\") < 30), \"Medium (10-30 km/h)\")\n",
    "    .otherwise(\"High (>30 km/h)\")\n",
    ")\n",
    "\n",
    "# E. Trip Reliability Indicators (Requirement: trip reliability indicators)\n",
    "# We classify reliability based on SRI (Service Reliability Index)\n",
    "# Assuming Higher SRI = Better reliability/Less congestion (based on data context)\n",
    "# or if SRI is delay-based (0 is on time), we adapt. \n",
    "# Looking at data: High congestion has High SRI (e.g., 5.14). Smooth has lower (e.g. 1.2 or -0.4).\n",
    "# Thus: High SRI = Unreliable (Delayed/Congested).\n",
    "\n",
    "df_features = df_features.withColumn(\n",
    "    \"reliability_status\",\n",
    "    when(col(\"SRI\") > 2, \"Unreliable (Congested)\")\n",
    "    .otherwise(\"Reliable\")\n",
    ")\n",
    "\n",
    "print(\"Features Engineered successfully (including Reliability Indicators).\")\n",
    "df_features.select(\"arrival_time\", \"is_peak\", \"SRI\", \"reliability_status\", \"speed_band\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21cb1ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Route Stats:\n",
      "+--------------------+------------------+-------------------+------------------+-------------+\n",
      "|             trip_id|         avg_speed|            avg_sri|    sri_volatility|total_records|\n",
      "+--------------------+------------------+-------------------+------------------+-------------+\n",
      "|NORMAL_315_Bhosar...|23.314078886235293| 1.4234352720588235|7.1211235313060115|           17|\n",
      "|NORMAL_64_Hadapsa...|22.961665740333334|  3.622047246722221|2.1446214256149636|           18|\n",
      "|NORMAL_43_Katraj ...|    48.62421636025|-5.7648642178000005|22.019612649346772|           40|\n",
      "|NORMAL_337_Bhakti...|28.021119715749997|      3.02426024225|3.7239498950763354|            8|\n",
      "|NORMAL_319_Alandi...|39.565834718076935|       -4.555626899| 5.109305275344527|           26|\n",
      "+--------------------+------------------+-------------------+------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Route Level Aggregation (Requirement: route-level aggregates)\n",
    "# Calculating stats per route to save as a separate dataset for dashboards\n",
    "route_stats = df_features.groupBy(\"trip_id\").agg(\n",
    "    avg(\"speed\").alias(\"avg_speed\"),\n",
    "    avg(\"SRI\").alias(\"avg_sri\"),\n",
    "    stddev(\"SRI\").alias(\"sri_volatility\"),\n",
    "    count(\"*\").alias(\"total_records\")\n",
    ")\n",
    "\n",
    "print(\"Sample Route Stats:\")\n",
    "route_stats.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04ad128b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Silver Dataset to HDFS: hdfs://namenode:9000/terraflow/data/processed/gtfs_silver.parquet\n",
      "✓ Silver layer saved successfully.\n",
      "Saving Route Stats to HDFS: hdfs://namenode:9000/terraflow/data/processed/route_stats.parquet\n",
      "✓ Route Stats saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# 5. Save Datasets to HDFS\n",
    "\n",
    "# Save Silver Layer (Main Dataset)\n",
    "print(f\"Saving Silver Dataset to HDFS: {SILVER_OUTPUT_PATH}\")\n",
    "df_features.write.mode(\"overwrite\").partitionBy(\"is_peak\").parquet(SILVER_OUTPUT_PATH)\n",
    "print(\"✓ Silver layer saved successfully.\")\n",
    "\n",
    "# Save Route Stats (Aggregated Dataset for Dashboard)\n",
    "print(f\"Saving Route Stats to HDFS: {ROUTE_STATS_OUTPUT_PATH}\")\n",
    "route_stats.write.mode(\"overwrite\").parquet(ROUTE_STATS_OUTPUT_PATH)\n",
    "print(\"✓ Route Stats saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80b00b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying Silver Layer integrity...\n",
      "Total Rows: 66,437\n",
      "\n",
      "Verifying Route Stats integrity...\n",
      "Total Routes: 5,332\n"
     ]
    }
   ],
   "source": [
    "# 6. Verification\n",
    "print(\"Verifying Silver Layer integrity...\")\n",
    "df_silver = spark.read.parquet(SILVER_OUTPUT_PATH)\n",
    "print(f\"Total Rows: {df_silver.count():,}\")\n",
    "\n",
    "print(\"\\nVerifying Route Stats integrity...\")\n",
    "df_stats = spark.read.parquet(ROUTE_STATS_OUTPUT_PATH)\n",
    "print(f\"Total Routes: {df_stats.count():,}\")\n",
    "\n",
    "# Spark Stop\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
