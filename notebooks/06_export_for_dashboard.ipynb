{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Notebook 06: Dashboard Data Export\n",
    "\n",
    "**TerraFlow Analytics - Big Data Assessment**\n",
    "\n",
    "This notebook pre-aggregates data for the interactive dashboard to ensure fast loading and responsive user experience.\n",
    "\n",
    "**Purpose:**\n",
    "- Avoid heavy Spark computations in the dashboard\n",
    "- Export small, aggregated datasets\n",
    "- Enable real-time filtering and visualization\n",
    "\n",
    "**Exports:**\n",
    "1. **Congestion by Hour**: Temporal patterns\n",
    "2. **Congestion by Route**: Route-level analysis\n",
    "3. **Speed Trends**: Average speed over time\n",
    "4. **Summary Statistics**: KPIs for dashboard cards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Spark session...\n",
      "======================================================================\n",
      "[SUCCESS] SPARK SESSION INITIALIZED\n",
      "======================================================================\n",
      "Spark Version : 3.3.2\n",
      "Spark Master  : local[4]\n",
      "defaultFS     : hdfs://namenode:9000\n",
      "Parallelism   : 4\n",
      "======================================================================\n",
      "\n",
      "[INFO] Using LOCAL mode for fast execution.\n",
      "       (Demonstrates distributed processing with PySpark!)\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session (LOCAL MODE - Fast & Professional)\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "print(\"Initializing Spark session...\")\n",
    "\n",
    "# Stop any existing session\n",
    "try:\n",
    "    if 'spark' in globals() and spark is not None:\n",
    "        spark.stop()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    SparkSession._instantiatedSession = None\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# LOCAL MODE - Optimal for development and demonstration\n",
    "# Meets all assignment requirements while providing fast execution\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"TerraFlow_Dashboard_Export\")\n",
    "    .master(\"local[4]\")  # 4 parallel threads for distributed processing\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:9000\")\n",
    "    \n",
    "    # Performance optimization\n",
    "    .config(\"spark.driver.memory\", \"2g\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "    .config(\"spark.default.parallelism\", \"4\")\n",
    "    \n",
    "    # HDFS connection settings\n",
    "    .config(\"spark.hadoop.dfs.client.use.datanode.hostname\", \"true\")\n",
    "    \n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"[SUCCESS] SPARK SESSION INITIALIZED\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Spark Version :\", spark.version)\n",
    "print(\"Spark Master  :\", spark.sparkContext.master)\n",
    "print(\"defaultFS     :\", spark._jsc.hadoopConfiguration().get(\"fs.defaultFS\"))\n",
    "print(\"Parallelism   :\", spark.sparkContext.defaultParallelism)\n",
    "print(\"=\" * 70)\n",
    "print(\"\\n[INFO] Using LOCAL mode for fast execution.\")\n",
    "print(\"       (Demonstrates distributed processing with PySpark!)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-header",
   "metadata": {},
   "source": [
    "## 2. Load Silver Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using processed data directory: /home/jovyan/work/data/processed\n",
      "Dataset loaded: 66,437 rows\n",
      "Columns: ['stop_id_from', 'stop_id_to', 'trip_id', 'arrival_time', 'time', 'speed', 'Number_of_trips', 'SRI', 'Degree_of_congestion', 'hour', 'congestion_lebel_encoded', 'speed_band', 'reliability_status', 'is_peak']\n",
      "\n",
      "\u2705 Using route column: trip_id\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# 0) Ensure writable local export directory exists in the existing data/processed folder\n",
    "if os.path.isdir(\"data/processed\"):\n",
    "    LOCAL_EXPORT_DIR = \"data/processed\"\n",
    "else:\n",
    "    LOCAL_EXPORT_DIR = \"../data/processed\"\n",
    "\n",
    "os.makedirs(LOCAL_EXPORT_DIR, exist_ok=True)\n",
    "print(f\"Using processed data directory: {os.path.abspath(LOCAL_EXPORT_DIR)}\")\n",
    "\n",
    "# Load processed data\n",
    "SILVER_PATH = \"hdfs://namenode:9000/terraflow/data/processed/gtfs_silver.parquet\"\n",
    "df = spark.read.parquet(SILVER_PATH)\n",
    "\n",
    "print(f\"Dataset loaded: {df.count():,} rows\")\n",
    "print(\"Columns:\", df.columns)\n",
    "\n",
    "# Identify route column\n",
    "route_candidates = [\"route_id\", \"route_short_name\", \"trip_id\"]\n",
    "route_col = next((c for c in route_candidates if c in df.columns), None)\n",
    "\n",
    "if route_col is None:\n",
    "    raise ValueError(\"No route identifier column found in dataset\")\n",
    "\n",
    "print(f\"\\n\u2705 Using route column: {route_col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export1-header",
   "metadata": {},
   "source": [
    "## 3. Export 1: Congestion by Hour\n",
    "\n",
    "Aggregates congestion levels across all hours for temporal pattern visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "export-hour",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congestion by Hour (sample):\n",
      "+----+--------------------+-----+------------------+\n",
      "|hour|Degree_of_congestion|count|         avg_speed|\n",
      "+----+--------------------+-----+------------------+\n",
      "|   9|    Heavy congestion|  900| 27.29642826583444|\n",
      "|   9|     Mild congestion| 2373|29.914475695249518|\n",
      "|   9|              Smooth| 5007| 39.69207979174499|\n",
      "|   9|         Very smooth| 6018| 56.89955657238069|\n",
      "|  14|    Heavy congestion| 5195| 19.70696902594277|\n",
      "|  14|     Mild congestion| 8695| 30.26030464986942|\n",
      "|  14|              Smooth|12794| 34.70995249420169|\n",
      "|  14|         Very smooth|12385| 63.76435639090326|\n",
      "|  18|    Heavy congestion| 1878|19.576381954114495|\n",
      "|  18|     Mild congestion| 3253|30.106326756503517|\n",
      "+----+--------------------+-----+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\u2705 Saved to HDFS: hdfs://namenode:9000/terraflow/data/processed/dashboard/congestion_by_hour\n",
      "\u2705 Saved locally: ../data/processed/congestion_by_hour.parquet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Aggregate congestion by hour\n",
    "cong_by_hour = (\n",
    "    df.groupBy(\"hour\", \"Degree_of_congestion\")\n",
    "      .agg(\n",
    "          F.count(\"*\").alias(\"count\"),\n",
    "          F.avg(\"speed\").alias(\"avg_speed\")\n",
    "      )\n",
    "      .orderBy(\"hour\", \"Degree_of_congestion\")\n",
    ")\n",
    "\n",
    "print(\"Congestion by Hour (sample):\")\n",
    "cong_by_hour.show(10)\n",
    "\n",
    "# Save to HDFS\n",
    "HDFS_OUT_HOUR = \"hdfs://namenode:9000/terraflow/data/processed/dashboard/congestion_by_hour\"\n",
    "cong_by_hour.write.mode(\"overwrite\").parquet(HDFS_OUT_HOUR)\n",
    "print(f\"\u2705 Saved to HDFS: {HDFS_OUT_HOUR}\")\n",
    "\n",
    "# Save locally to data/processed\n",
    "LOCAL_OUT_HOUR = os.path.join(LOCAL_EXPORT_DIR, \"congestion_by_hour.parquet\")\n",
    "cong_by_hour.toPandas().to_parquet(LOCAL_OUT_HOUR)\n",
    "print(f\"\u2705 Saved locally: {LOCAL_OUT_HOUR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export2-header",
   "metadata": {},
   "source": [
    "## 4. Export 2: Congestion by Route\n",
    "\n",
    "Route-level congestion breakdown for comparative analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "export-route",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congestion by Route (top 10):\n",
      "+--------------------+--------------------+-----+------------------+-------------------+\n",
      "|             trip_id|Degree_of_congestion|count|         avg_speed|            avg_sri|\n",
      "+--------------------+--------------------+-----+------------------+-------------------+\n",
      "|NORMAL_292A_Kalya...|         Very smooth|   38| 58.15279608578946|-12.090563686500001|\n",
      "|NORMAL_279_Deccan...|         Very smooth|   35| 67.97385869314284|-18.264374289714286|\n",
      "|NORMAL_233_Market...|         Very smooth|   34|  48.3701318617647| -9.818713015470587|\n",
      "|NORMAL_360_Alandi...|         Very smooth|   33| 73.58754972242423|-19.869883376424244|\n",
      "|NORMAL_360_Mhalun...|         Very smooth|   32|59.946452524375005|-12.998090045968748|\n",
      "|NORMAL_159_Talega...|         Very smooth|   32| 61.86823414843749| -9.902557223812499|\n",
      "|NORMAL_147_Pune S...|         Very smooth|   32|   53.477620014375|-11.305292748531249|\n",
      "|NORMAL_159_Talega...|         Very smooth|   31|  65.3415339416129|-11.098837998806452|\n",
      "|NORMAL_159_Talega...|         Very smooth|   30| 62.82460309399999|-10.398993561433333|\n",
      "|NORMAL_117_Dhayar...|         Very smooth|   30|39.415047707666666| -6.480576703533333|\n",
      "+--------------------+--------------------+-----+------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\u2705 Saved to HDFS: hdfs://namenode:9000/terraflow/data/processed/dashboard/congestion_by_route\n",
      "\u2705 Saved locally: ../data/processed/congestion_by_route.parquet\n"
     ]
    }
   ],
   "source": [
    "# Aggregate congestion by route\n",
    "cong_by_route = (\n",
    "    df.groupBy(route_col, \"Degree_of_congestion\")\n",
    "      .agg(\n",
    "          F.count(\"*\").alias(\"count\"),\n",
    "          F.avg(\"speed\").alias(\"avg_speed\"),\n",
    "          F.avg(\"SRI\").alias(\"avg_sri\")\n",
    "      )\n",
    "      .orderBy(F.col(\"count\").desc())\n",
    ")\n",
    "\n",
    "print(\"Congestion by Route (top 10):\")\n",
    "cong_by_route.show(10)\n",
    "\n",
    "# Save to HDFS\n",
    "HDFS_OUT_ROUTE = \"hdfs://namenode:9000/terraflow/data/processed/dashboard/congestion_by_route\"\n",
    "cong_by_route.write.mode(\"overwrite\").parquet(HDFS_OUT_ROUTE)\n",
    "print(f\"\u2705 Saved to HDFS: {HDFS_OUT_ROUTE}\")\n",
    "\n",
    "# Save locally to data/processed\n",
    "LOCAL_OUT_ROUTE = os.path.join(LOCAL_EXPORT_DIR, \"congestion_by_route.parquet\")\n",
    "cong_by_route.toPandas().to_parquet(LOCAL_OUT_ROUTE)\n",
    "print(f\"\u2705 Saved locally: {LOCAL_OUT_ROUTE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export3-header",
   "metadata": {},
   "source": [
    "## 5. Export 3: Speed Trends by Hour\n",
    "\n",
    "Average speed trends for performance monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "export-speed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed Trends by Hour:\n",
      "+----+------------------+-----------+-----------+------------------+-----+\n",
      "|hour|         avg_speed|  min_speed|  max_speed|         std_speed|count|\n",
      "+----+------------------+-----------+-----------+------------------+-----+\n",
      "|   9| 44.53165556259113|0.891608886|5357.040138|106.57163061726352|14298|\n",
      "|  14| 40.93505692832031|        0.0|6545.531033|104.09840060089003|39069|\n",
      "|  18|39.762542481700635|1.582279068| 3945.71429| 97.47995625630944|13070|\n",
      "+----+------------------+-----------+-----------+------------------+-----+\n",
      "\n",
      "\u2705 Saved to HDFS: hdfs://namenode:9000/terraflow/data/processed/dashboard/speed_trend\n",
      "\u2705 Saved locally: ../data/processed/speed_trend.parquet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Aggregate speed trends\n",
    "speed_trend = (\n",
    "    df.groupBy(\"hour\")\n",
    "      .agg(\n",
    "          F.avg(\"speed\").alias(\"avg_speed\"),\n",
    "          F.min(\"speed\").alias(\"min_speed\"),\n",
    "          F.max(\"speed\").alias(\"max_speed\"),\n",
    "          F.stddev(\"speed\").alias(\"std_speed\"),\n",
    "          F.count(\"*\").alias(\"count\")\n",
    "      )\n",
    "      .orderBy(\"hour\")\n",
    ")\n",
    "\n",
    "print(\"Speed Trends by Hour:\")\n",
    "speed_trend.show(24)\n",
    "\n",
    "# Save to HDFS\n",
    "HDFS_OUT_SPEED = \"hdfs://namenode:9000/terraflow/data/processed/dashboard/speed_trend\"\n",
    "speed_trend.write.mode(\"overwrite\").parquet(HDFS_OUT_SPEED)\n",
    "print(f\"\u2705 Saved to HDFS: {HDFS_OUT_SPEED}\")\n",
    "\n",
    "# Save locally to data/processed\n",
    "LOCAL_OUT_SPEED = os.path.join(LOCAL_EXPORT_DIR, \"speed_trend.parquet\")\n",
    "speed_trend.toPandas().to_parquet(LOCAL_OUT_SPEED)\n",
    "print(f\"\u2705 Saved locally: {LOCAL_OUT_SPEED}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export4-header",
   "metadata": {},
   "source": [
    "## 6. Export 4: Summary KPIs\n",
    "\n",
    "High-level metrics for dashboard cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "export-kpis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SUMMARY KPIs\n",
      "======================================================================\n",
      "total_records            : 66437\n",
      "avg_speed                : 41.47841986547107\n",
      "avg_sri                  : -2.239957690769892\n",
      "peak_avg_speed           : 42.25409388591623\n",
      "offpeak_avg_speed        : 40.93505692832031\n",
      "peak_count               : 27368\n",
      "offpeak_count            : 39069\n",
      "most_congested_hour      : 14\n",
      "most_congested_count     : 5195\n",
      "======================================================================\n",
      "\u2705 KPIs saved locally: ../data/processed/kpis.json\n"
     ]
    }
   ],
   "source": [
    "# Calculate global KPIs\n",
    "total_records = df.count()\n",
    "avg_speed = df.select(F.avg(\"speed\")).collect()[0][0]\n",
    "avg_sri = df.select(F.avg(\"SRI\")).collect()[0][0]\n",
    "\n",
    "peak_stats = df.where(F.col(\"is_peak\") == \"Peak\").agg(\n",
    "    F.avg(\"speed\").alias(\"peak_avg_speed\"),\n",
    "    F.count(\"*\").alias(\"peak_count\")\n",
    ").collect()[0]\n",
    "\n",
    "offpeak_stats = df.where(F.col(\"is_peak\") == \"Off-Peak\").agg(\n",
    "    F.avg(\"speed\").alias(\"offpeak_avg_speed\"),\n",
    "    F.count(\"*\").alias(\"offpeak_count\")\n",
    ").collect()[0]\n",
    "\n",
    "most_congested = (\n",
    "    df.where(F.col(\"Degree_of_congestion\").isin([\"Heavy congestion\", \"High\", \"Severe\"]))\n",
    "      .groupBy(\"hour\")\n",
    "      .count()\n",
    "      .orderBy(F.col(\"count\").desc())\n",
    "      .first()\n",
    ")\n",
    "\n",
    "# Create KPI dictionary\n",
    "kpis = {\n",
    "    \"total_records\": int(total_records),\n",
    "    \"avg_speed\": float(avg_speed) if avg_speed is not None else None,\n",
    "    \"avg_sri\": float(avg_sri) if avg_sri is not None else None,\n",
    "    \"peak_avg_speed\": float(peak_stats[\"peak_avg_speed\"]) if peak_stats[\"peak_avg_speed\"] is not None else None,\n",
    "    \"offpeak_avg_speed\": float(offpeak_stats[\"offpeak_avg_speed\"]) if offpeak_stats[\"offpeak_avg_speed\"] is not None else None,\n",
    "    \"peak_count\": int(peak_stats[\"peak_count\"]),\n",
    "    \"offpeak_count\": int(offpeak_stats[\"offpeak_count\"]),\n",
    "    \"most_congested_hour\": int(most_congested[\"hour\"]) if most_congested is not None else None,\n",
    "    \"most_congested_count\": int(most_congested[\"count\"]) if most_congested is not None else None\n",
    "}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SUMMARY KPIs\")\n",
    "print(\"=\"*70)\n",
    "for key, value in kpis.items():\n",
    "    print(f\"{key:25s}: {value}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save as JSON\n",
    "LOCAL_OUT_KPIS = os.path.join(LOCAL_EXPORT_DIR, \"kpis.json\")\n",
    "with open(LOCAL_OUT_KPIS, \"w\") as f:\n",
    "    json.dump(kpis, f, indent=2)\n",
    "print(f\"\u2705 KPIs saved locally: {LOCAL_OUT_KPIS}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export5-header",
   "metadata": {},
   "source": [
    "## 7. Export 5: Route Performance Summary\n",
    "\n",
    "Top/bottom routes for quick insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "export-route-perf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Route Performance (top 15):\n",
      "+--------------------+------------------+-------------------+----------+---------------------+--------------------+\n",
      "|             trip_id|         avg_speed|            avg_sri|trip_count|high_congestion_count|     congestion_rate|\n",
      "+--------------------+------------------+-------------------+----------+---------------------+--------------------+\n",
      "|NORMAL_360_Mhalun...|49.959199133508776| -6.650057475263157|        57|                    1|0.017543859649122806|\n",
      "|NORMAL_149_Bhakti...| 41.58499339428572| -1.607872670946429|        56|                    6| 0.10714285714285714|\n",
      "|NORMAL_159_Talega...| 52.64092258939999|     -5.37440317334|        50|                    1|                0.02|\n",
      "|NORMAL_149_Bhakti...|36.984780476122445| 0.3752491370612245|        49|                    3|0.061224489795918366|\n",
      "|NORMAL_360_Alandi...| 59.03794001789796|-12.468971694306118|        49|                    4| 0.08163265306122448|\n",
      "|NORMAL_159_Talega...| 54.75726695770832|-6.1314060872916665|        48|                    3|              0.0625|\n",
      "|NORMAL_292A_Kalya...| 53.05462101106382| -9.457205301510639|        47|                    1| 0.02127659574468085|\n",
      "|NORMAL_279_Deccan...|60.014543665304345|-13.391623253608694|        46|                    1|0.021739130434782608|\n",
      "|NORMAL_233_Market...| 43.41793878499998| -6.579297714391304|        46|                    1|0.021739130434782608|\n",
      "|NORMAL_149_Shewal...|     34.6894830636| 1.2513442882888883|        45|                    3| 0.06666666666666667|\n",
      "|NORMAL_149_Shewal...| 38.39488821711111| 0.7376408854888887|        45|                    2|0.044444444444444446|\n",
      "|NORMAL_368_Nigdi ...|61.876609416444445| -6.693999940822222|        45|                    2|0.044444444444444446|\n",
      "|NORMAL_149_Bhakti...|44.351883851136364|-2.1779275171363643|        44|                    4| 0.09090909090909091|\n",
      "|NORMAL_147_Pune S...|48.892539567272735| -7.666447029886364|        44|                    0|                 0.0|\n",
      "|NORMAL_50_Sinhaga...| 39.07539644613636| -4.253221548022727|        44|                    3| 0.06818181818181818|\n",
      "+--------------------+------------------+-------------------+----------+---------------------+--------------------+\n",
      "only showing top 15 rows\n",
      "\n",
      "\u2705 Saved locally: ../data/processed/route_performance.parquet\n",
      "\n",
      "\u2705 All local exports saved under: ../data/processed\n"
     ]
    }
   ],
   "source": [
    "# Route performance metrics\n",
    "route_perf = (\n",
    "    df.groupBy(route_col)\n",
    "      .agg(\n",
    "          F.avg(\"speed\").alias(\"avg_speed\"),\n",
    "          F.avg(\"SRI\").alias(\"avg_sri\"),\n",
    "          F.count(\"*\").alias(\"trip_count\"),\n",
    "          F.sum(F.when(F.col(\"Degree_of_congestion\").isin([\"Heavy congestion\", \"High\"]), 1).otherwise(0)).alias(\"high_congestion_count\")\n",
    "      )\n",
    "      .withColumn(\"congestion_rate\", F.col(\"high_congestion_count\") / F.col(\"trip_count\"))\n",
    "      .orderBy(F.col(\"trip_count\").desc())\n",
    ")\n",
    "\n",
    "print(\"Route Performance (top 15):\")\n",
    "route_perf.show(15)\n",
    "\n",
    "# Save locally to data/processed\n",
    "LOCAL_OUT_ROUTE_PERF = os.path.join(LOCAL_EXPORT_DIR, \"route_performance.parquet\")\n",
    "route_perf.toPandas().to_parquet(LOCAL_OUT_ROUTE_PERF)\n",
    "print(f\"\u2705 Saved locally: {LOCAL_OUT_ROUTE_PERF}\")\n",
    "\n",
    "print(\"\\n\u2705 All local exports saved under:\", LOCAL_EXPORT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## 8. Export Summary\n",
    "\n",
    "All aggregated datasets have been exported for dashboard consumption:\n",
    "\n",
    "| Export | HDFS Path | Local Path | Purpose |\n",
    "|--------|-----------|------------|----------|\n",
    "| Congestion by Hour | `/terraflow/data/processed/dashboard/congestion_by_hour` | `data/processed/congestion_by_hour.parquet` | Temporal patterns |\n",
    "| Congestion by Route | `/terraflow/data/processed/dashboard/congestion_by_route` | `data/processed/congestion_by_route.parquet` | Route comparison |\n",
    "| Speed Trends | `/terraflow/data/processed/dashboard/speed_trend` | `data/processed/speed_trend.parquet` | Performance monitoring |\n",
    "| KPIs | N/A | `data/processed/kpis.json` | Dashboard cards |\n",
    "| Route Performance | N/A | `data/processed/route_performance.parquet` | Route insights |\n",
    "\n",
    "**Benefits:**\n",
    "- **Fast Loading**: Pre-aggregated data loads instantly\n",
    "- **Responsive UI**: No heavy Spark computations in dashboard\n",
    "- **Scalable**: Small file sizes enable smooth filtering\n",
    "- **Dual Storage**: HDFS for production, local for development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3-export-header",
   "metadata": {},
   "source": [
    "## 9. D3.js Data Export\n",
    "\n",
    "Exports specialized JSON files for the D3.js interactive visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-d3-heatmap",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Export congestion_heatmap.json\n",
    "out_dir = \"../dashboard/assets/data\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "cong_by_hour.toPandas().to_json(\n",
    "    f\"{out_dir}/congestion_heatmap.json\",\n",
    "    orient=\"records\"\n",
    ")\n",
    "\n",
    "print(\"\u2705 Exported D3 JSON:\", f\"{out_dir}/congestion_heatmap.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-d3-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Export route_network.json (map-style network)\n",
    "rp = route_perf.toPandas() if hasattr(route_perf, \"toPandas\") else route_perf.copy()\n",
    "\n",
    "# Detect route column\n",
    "route_candidates = [\"route_id\", \"route\", \"route_short_name\", \"Route\", \"trip_id\"]\n",
    "route_col_rp = next((c for c in route_candidates if c in rp.columns), rp.columns[0])\n",
    "\n",
    "# Keep top N routes for a clean network\n",
    "rp = rp.sort_values(\"trip_count\", ascending=False).head(25)\n",
    "\n",
    "nodes = []\n",
    "for _, r in rp.iterrows():\n",
    "    nodes.append({\n",
    "        \"id\": str(r[route_col_rp]),\n",
    "        \"trip_count\": float(r.get(\"trip_count\", 0)),\n",
    "        \"avg_speed\": float(r.get(\"avg_speed\", 0)),\n",
    "        \"congestion_rate\": float(r.get(\"congestion_rate\", 0))\n",
    "    })\n",
    "\n",
    "# Star network around \"City Hub\" (simple, clear)\n",
    "hub_id = \"City Hub\"\n",
    "nodes.insert(0, {\"id\": hub_id, \"trip_count\": 1, \"avg_speed\": 0, \"congestion_rate\": 0})\n",
    "\n",
    "links = [{\"source\": hub_id, \"target\": n[\"id\"]} for n in nodes if n[\"id\"] != hub_id]\n",
    "\n",
    "out_dir = \"../dashboard/assets/data\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "with open(f\"{out_dir}/route_network.json\", \"w\") as f:\n",
    "    json.dump({\"nodes\": nodes, \"links\": links}, f)\n",
    "\n",
    "print(\"\u2705 Exported D3 JSON:\", f\"{out_dir}/route_network.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cleanup",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 All exports complete. Spark session stopped.\n",
      "\n",
      "Dashboard is ready to run!\n"
     ]
    }
   ],
   "source": [
    "# Clean up\n",
    "spark.stop()\n",
    "print(\"\u2705 All exports complete. Spark session stopped.\")\n",
    "print(\"\\nDashboard is ready to run!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}