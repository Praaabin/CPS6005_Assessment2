{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "header",
            "metadata": {},
            "source": [
                "# Notebook 06: Dashboard Data Export\n",
                "\n",
                "**TerraFlow Analytics - Big Data Assessment**\n",
                "\n",
                "This notebook pre-aggregates data for the interactive dashboard to ensure fast loading and responsive user experience.\n",
                "\n",
                "**Purpose:**\n",
                "- Avoid heavy Spark computations in the dashboard\n",
                "- Export small, aggregated datasets\n",
                "- Enable real-time filtering and visualization\n",
                "\n",
                "**Exports:**\n",
                "1. **Congestion by Hour**: Temporal patterns\n",
                "2. **Congestion by Route**: Route-level analysis\n",
                "3. **Speed Trends**: Average speed over time\n",
                "4. **Summary Statistics**: KPIs for dashboard cards"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "setup-header",
            "metadata": {},
            "source": [
                "## 1. Environment Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.sql import SparkSession\n",
                "import pyspark.sql.functions as F\n",
                "import os\n",
                "\n",
                "# Initialize Spark Session\n",
                "spark = (\n",
                "    SparkSession.builder\n",
                "    .appName(\"TerraFlow_Dashboard_Export\")\n",
                "    .master(\"local[*]\")\n",
                "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:9000\")\n",
                "    .config(\"spark.sql.shuffle.partitions\", \"8\")\n",
                "    .getOrCreate()\n",
                ")\n",
                "spark.sparkContext.setLogLevel(\"WARN\")\n",
                "print(\"✅ Spark Session Initialized\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "load-header",
            "metadata": {},
            "source": [
                "## 2. Load Silver Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "load-data",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load processed data\n",
                "SILVER_PATH = \"hdfs://namenode:9000/terraflow/data/processed/gtfs_silver.parquet\"\n",
                "df = spark.read.parquet(SILVER_PATH)\n",
                "\n",
                "print(f\"Dataset loaded: {df.count():,} rows\")\n",
                "print(\"Columns:\", df.columns)\n",
                "\n",
                "# Identify route column\n",
                "route_candidates = [\"route_id\", \"route_short_name\", \"trip_id\"]\n",
                "route_col = next((c for c in route_candidates if c in df.columns), None)\n",
                "\n",
                "if route_col is None:\n",
                "    raise ValueError(\"No route identifier column found in dataset\")\n",
                "\n",
                "print(f\"\\n✅ Using route column: {route_col}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "export1-header",
            "metadata": {},
            "source": [
                "## 3. Export 1: Congestion by Hour\n",
                "\n",
                "Aggregates congestion levels across all hours for temporal pattern visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "export-hour",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Aggregate congestion by hour\n",
                "cong_by_hour = (\n",
                "    df.groupBy(\"hour\", \"Degree_of_congestion\")\n",
                "    .agg(\n",
                "        F.count(\"*\").alias(\"count\"),\n",
                "        F.avg(\"speed\").alias(\"avg_speed\")\n",
                "    )\n",
                "    .orderBy(\"hour\", \"Degree_of_congestion\")\n",
                ")\n",
                "\n",
                "print(\"Congestion by Hour (sample):\")\n",
                "cong_by_hour.show(10)\n",
                "\n",
                "# Save to HDFS\n",
                "HDFS_OUT_HOUR = \"hdfs://namenode:9000/terraflow/data/processed/dashboard/congestion_by_hour\"\n",
                "cong_by_hour.write.mode(\"overwrite\").parquet(HDFS_OUT_HOUR)\n",
                "print(f\"✅ Saved to HDFS: {HDFS_OUT_HOUR}\")\n",
                "\n",
                "# Also save locally for dashboard\n",
                "LOCAL_OUT_HOUR = \"../data/processed/congestion_by_hour.parquet\"\n",
                "cong_by_hour.toPandas().to_parquet(LOCAL_OUT_HOUR)\n",
                "print(f\"✅ Saved locally: {LOCAL_OUT_HOUR}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "export2-header",
            "metadata": {},
            "source": [
                "## 4. Export 2: Congestion by Route\n",
                "\n",
                "Route-level congestion breakdown for comparative analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "export-route",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Aggregate congestion by route\n",
                "cong_by_route = (\n",
                "    df.groupBy(route_col, \"Degree_of_congestion\")\n",
                "    .agg(\n",
                "        F.count(\"*\").alias(\"count\"),\n",
                "        F.avg(\"speed\").alias(\"avg_speed\"),\n",
                "        F.avg(\"SRI\").alias(\"avg_sri\")\n",
                "    )\n",
                "    .orderBy(F.col(\"count\").desc())\n",
                ")\n",
                "\n",
                "print(\"Congestion by Route (top 10):\")\n",
                "cong_by_route.show(10)\n",
                "\n",
                "# Save to HDFS\n",
                "HDFS_OUT_ROUTE = \"hdfs://namenode:9000/terraflow/data/processed/dashboard/congestion_by_route\"\n",
                "cong_by_route.write.mode(\"overwrite\").parquet(HDFS_OUT_ROUTE)\n",
                "print(f\"✅ Saved to HDFS: {HDFS_OUT_ROUTE}\")\n",
                "\n",
                "# Save locally\n",
                "LOCAL_OUT_ROUTE = \"../data/processed/congestion_by_route.parquet\"\n",
                "cong_by_route.toPandas().to_parquet(LOCAL_OUT_ROUTE)\n",
                "print(f\"✅ Saved locally: {LOCAL_OUT_ROUTE}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "export3-header",
            "metadata": {},
            "source": [
                "## 5. Export 3: Speed Trends by Hour\n",
                "\n",
                "Average speed trends for performance monitoring"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "export-speed",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Aggregate speed trends\n",
                "speed_trend = (\n",
                "    df.groupBy(\"hour\")\n",
                "    .agg(\n",
                "        F.avg(\"speed\").alias(\"avg_speed\"),\n",
                "        F.min(\"speed\").alias(\"min_speed\"),\n",
                "        F.max(\"speed\").alias(\"max_speed\"),\n",
                "        F.stddev(\"speed\").alias(\"std_speed\"),\n",
                "        F.count(\"*\").alias(\"count\")\n",
                "    )\n",
                "    .orderBy(\"hour\")\n",
                ")\n",
                "\n",
                "print(\"Speed Trends by Hour:\")\n",
                "speed_trend.show(24)\n",
                "\n",
                "# Save to HDFS\n",
                "HDFS_OUT_SPEED = \"hdfs://namenode:9000/terraflow/data/processed/dashboard/speed_trend\"\n",
                "speed_trend.write.mode(\"overwrite\").parquet(HDFS_OUT_SPEED)\n",
                "print(f\"✅ Saved to HDFS: {HDFS_OUT_SPEED}\")\n",
                "\n",
                "# Save locally\n",
                "LOCAL_OUT_SPEED = \"../data/processed/speed_trend.parquet\"\n",
                "speed_trend.toPandas().to_parquet(LOCAL_OUT_SPEED)\n",
                "print(f\"✅ Saved locally: {LOCAL_OUT_SPEED}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "export4-header",
            "metadata": {},
            "source": [
                "## 6. Export 4: Summary KPIs\n",
                "\n",
                "High-level metrics for dashboard cards"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "export-kpis",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate global KPIs\n",
                "total_records = df.count()\n",
                "avg_speed = df.select(F.avg(\"speed\")).collect()[0][0]\n",
                "avg_sri = df.select(F.avg(\"SRI\")).collect()[0][0]\n",
                "\n",
                "# Peak vs off-peak comparison\n",
                "peak_stats = df.where(F.col(\"is_peak\") == \"Peak\").agg(\n",
                "    F.avg(\"speed\").alias(\"peak_avg_speed\"),\n",
                "    F.count(\"*\").alias(\"peak_count\")\n",
                ").collect()[0]\n",
                "\n",
                "offpeak_stats = df.where(F.col(\"is_peak\") == \"Off-Peak\").agg(\n",
                "    F.avg(\"speed\").alias(\"offpeak_avg_speed\"),\n",
                "    F.count(\"*\").alias(\"offpeak_count\")\n",
                ").collect()[0]\n",
                "\n",
                "# Most congested hour\n",
                "most_congested = (\n",
                "    df.where(F.col(\"Degree_of_congestion\").isin([\"Heavy congestion\", \"High\", \"Severe\"]))\n",
                "    .groupBy(\"hour\")\n",
                "    .count()\n",
                "    .orderBy(F.col(\"count\").desc())\n",
                "    .first()\n",
                ")\n",
                "\n",
                "# Create KPI dictionary\n",
                "kpis = {\n",
                "    \"total_records\": int(total_records),\n",
                "    \"avg_speed\": float(avg_speed),\n",
                "    \"avg_sri\": float(avg_sri),\n",
                "    \"peak_avg_speed\": float(peak_stats[\"peak_avg_speed\"]),\n",
                "    \"offpeak_avg_speed\": float(offpeak_stats[\"offpeak_avg_speed\"]),\n",
                "    \"peak_count\": int(peak_stats[\"peak_count\"]),\n",
                "    \"offpeak_count\": int(offpeak_stats[\"offpeak_count\"]),\n",
                "    \"most_congested_hour\": int(most_congested[\"hour\"]),\n",
                "    \"most_congested_count\": int(most_congested[\"count\"])\n",
                "}\n",
                "\n",
                "print(\"=\"*70)\n",
                "print(\"SUMMARY KPIs\")\n",
                "print(\"=\"*70)\n",
                "for key, value in kpis.items():\n",
                "    print(f\"{key:25s}: {value}\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "# Save as JSON\n",
                "import json\n",
                "LOCAL_OUT_KPIS = \"../data/processed/kpis.json\"\n",
                "with open(LOCAL_OUT_KPIS, 'w') as f:\n",
                "    json.dump(kpis, f, indent=2)\n",
                "print(f\"\\n✅ KPIs saved to: {LOCAL_OUT_KPIS}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "export5-header",
            "metadata": {},
            "source": [
                "## 7. Export 5: Route Performance Summary\n",
                "\n",
                "Top/bottom routes for quick insights"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "export-route-perf",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Route performance metrics\n",
                "route_perf = (\n",
                "    df.groupBy(route_col)\n",
                "    .agg(\n",
                "        F.avg(\"speed\").alias(\"avg_speed\"),\n",
                "        F.avg(\"SRI\").alias(\"avg_sri\"),\n",
                "        F.count(\"*\").alias(\"trip_count\"),\n",
                "        F.sum(F.when(F.col(\"Degree_of_congestion\").isin([\"Heavy congestion\", \"High\"]), 1).otherwise(0)).alias(\"high_congestion_count\")\n",
                "    )\n",
                "    .withColumn(\"congestion_rate\", F.col(\"high_congestion_count\") / F.col(\"trip_count\"))\n",
                "    .orderBy(F.col(\"trip_count\").desc())\n",
                ")\n",
                "\n",
                "print(\"Route Performance (top 15):\")\n",
                "route_perf.show(15)\n",
                "\n",
                "# Save locally\n",
                "LOCAL_OUT_ROUTE_PERF = \"../data/processed/route_performance.parquet\"\n",
                "route_perf.toPandas().to_parquet(LOCAL_OUT_ROUTE_PERF)\n",
                "print(f\"✅ Saved locally: {LOCAL_OUT_ROUTE_PERF}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "summary-header",
            "metadata": {},
            "source": [
                "## 8. Export Summary\n",
                "\n",
                "All aggregated datasets have been exported for dashboard consumption:\n",
                "\n",
                "| Export | HDFS Path | Local Path | Purpose |\n",
                "|--------|-----------|------------|----------|\n",
                "| Congestion by Hour | `/terraflow/data/processed/dashboard/congestion_by_hour` | `../data/processed/congestion_by_hour.parquet` | Temporal patterns |\n",
                "| Congestion by Route | `/terraflow/data/processed/dashboard/congestion_by_route` | `../data/processed/congestion_by_route.parquet` | Route comparison |\n",
                "| Speed Trends | `/terraflow/data/processed/dashboard/speed_trend` | `../data/processed/speed_trend.parquet` | Performance monitoring |\n",
                "| KPIs | N/A | `../data/processed/kpis.json` | Dashboard cards |\n",
                "| Route Performance | N/A | `../data/processed/route_performance.parquet` | Route insights |\n",
                "\n",
                "**Benefits:**\n",
                "- **Fast Loading**: Pre-aggregated data loads instantly\n",
                "- **Responsive UI**: No heavy Spark computations in dashboard\n",
                "- **Scalable**: Small file sizes enable smooth filtering\n",
                "- **Dual Storage**: HDFS for production, local for development"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cleanup",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clean up\n",
                "spark.stop()\n",
                "print(\"✅ All exports complete. Spark session stopped.\")\n",
                "print(\"\\nDashboard is ready to run!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.6"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}